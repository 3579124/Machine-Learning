{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDAMS ML Competition Solution\n",
    "\n",
    "## Competition Goal\n",
    "This notebook provides a comprehensive machine learning pipeline for the AIDAMS ML competition on Kaggle.\n",
    "\n",
    "**Objective:** Predict the academic risk of students in higher education\n",
    "\n",
    "**Target Variable:** Student outcome (Dropout, Enrolled, Graduate)\n",
    "\n",
    "**Approach:** We will compare three powerful gradient boosting models:\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Adjust these settings as needed\n",
    "# ============================================================\n",
    "\n",
    "# Model selection: 'xgboost', 'lightgbm', or 'catboost'\n",
    "SELECTED_MODEL = 'xgboost'\n",
    "\n",
    "# Enable/disable hyperparameter tuning (can take a long time)\n",
    "ENABLE_HYPERPARAMETER_TUNING = False\n",
    "\n",
    "# Enable/disable visualizations\n",
    "ENABLE_VISUALIZATIONS = True\n",
    "\n",
    "# Enable/disable feature engineering\n",
    "ENABLE_FEATURE_ENGINEERING = False\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Train-test split ratio\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Cross-validation folds\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Data file paths\n",
    "TRAIN_DATA_PATH = 'data.csv'\n",
    "TEST_DATA_PATH = 'test.csv'\n",
    "\n",
    "print(\"‚úì Configuration loaded successfully\")\n",
    "print(f\"  Selected Model: {SELECTED_MODEL}\")\n",
    "print(f\"  Hyperparameter Tuning: {ENABLE_HYPERPARAMETER_TUNING}\")\n",
    "print(f\"  Visualizations: {ENABLE_VISUALIZATIONS}\")\n",
    "print(f\"  Feature Engineering: {ENABLE_FEATURE_ENGINEERING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================================\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "# Models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading & Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "# Load the training data (handles both comma and semicolon separators)\n",
    "df = pd.read_csv(TRAIN_DATA_PATH, sep=None, engine='python')\n",
    "\n",
    "# Clean column names (remove tabs and extra spaces)\n",
    "df.columns = df.columns.str.replace('\\t', ' ').str.strip()\n",
    "\n",
    "print(\"‚úì Data loaded successfully\")\n",
    "print(f\"  Dataset shape: {df.shape}\")\n",
    "print(f\"  Number of samples: {df.shape[0]}\")\n",
    "print(f\"  Number of features: {df.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DISPLAY BASIC INFORMATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä Dataset Information:\")\n",
    "print(\"=\" * 60)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DISPLAY FIRST FEW ROWS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìã First 5 rows of the dataset:\")\n",
    "print(\"=\" * 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANALYZE MISSING VALUES\n",
    "# ============================================================\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è Missing Values Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"\\n‚úì No missing values found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TARGET VARIABLE DISTRIBUTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüéØ Target Variable Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "target_counts = df['Target'].value_counts()\n",
    "target_percent = (target_counts / len(df)) * 100\n",
    "\n",
    "target_summary = pd.DataFrame({\n",
    "    'Count': target_counts,\n",
    "    'Percentage': target_percent\n",
    "})\n",
    "\n",
    "print(target_summary)\n",
    "print(f\"\\nNumber of classes: {df['Target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BASIC STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìà Basic Statistical Summary:\")\n",
    "print(\"=\" * 60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TARGET VARIABLE DISTRIBUTION PLOT\n",
    "# ============================================================\n",
    "\n",
    "if ENABLE_VISUALIZATIONS:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    target_counts = df['Target'].value_counts()\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "    \n",
    "    plt.bar(target_counts.index, target_counts.values, color=colors[:len(target_counts)])\n",
    "    plt.xlabel('Target Class', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.title('Distribution of Target Variable', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (label, value) in enumerate(target_counts.items()):\n",
    "        plt.text(i, value, str(value), ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Target distribution plot saved as 'target_distribution.png'\")\n",
    "else:\n",
    "    print(\"‚äó Visualizations disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORRELATION HEATMAP FOR NUMERICAL FEATURES\n",
    "# ============================================================\n",
    "\n",
    "if ENABLE_VISUALIZATIONS:\n",
    "    # Select only numerical columns (excluding Target)\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Target' in numerical_cols:\n",
    "        numerical_cols.remove('Target')\n",
    "    \n",
    "    # Limit to top 20 features for better visualization\n",
    "    if len(numerical_cols) > 20:\n",
    "        numerical_cols = numerical_cols[:20]\n",
    "    \n",
    "    if len(numerical_cols) > 0:\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        correlation_matrix = df[numerical_cols].corr()\n",
    "        \n",
    "        sns.heatmap(correlation_matrix, \n",
    "                    annot=False, \n",
    "                    cmap='coolwarm', \n",
    "                    center=0,\n",
    "                    square=True,\n",
    "                    linewidths=0.5,\n",
    "                    cbar_kws={\"shrink\": 0.8})\n",
    "        \n",
    "        plt.title('Correlation Heatmap of Numerical Features', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úì Correlation heatmap saved as 'correlation_heatmap.png'\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No numerical features found for correlation analysis\")\n",
    "else:\n",
    "    print(\"‚äó Visualizations disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA PREPROCESSING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def preprocess_data(data, target_encoder=None, feature_encoders=None, is_training=True):\n",
    "    \"\"\"\n",
    "    Preprocess the data:\n",
    "    - Handle missing values\n",
    "    - Encode categorical variables\n",
    "    - Encode target variable (if training)\n",
    "    - Remove ID column\n",
    "    \n",
    "    Args:\n",
    "        data: Input DataFrame\n",
    "        target_encoder: LabelEncoder for target (used during prediction)\n",
    "        feature_encoders: Dictionary of LabelEncoders for features (used during prediction)\n",
    "        is_training: Whether this is training data\n",
    "    \n",
    "    Returns:\n",
    "        X: Processed features\n",
    "        y: Encoded labels (None if not training)\n",
    "        target_encoder: LabelEncoder for target\n",
    "        feature_encoders: Dictionary of LabelEncoders for features\n",
    "        feature_names: List of feature names\n",
    "    \"\"\"\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    # Separate target and features\n",
    "    if is_training:\n",
    "        if 'Target' not in df.columns:\n",
    "            raise ValueError(\"Target column not found in training data\")\n",
    "        y = df['Target'].copy()\n",
    "        X = df.drop(columns=['Target'])\n",
    "    else:\n",
    "        y = None\n",
    "        X = df.copy()\n",
    "    \n",
    "    # Remove ID column if present (but save it for later)\n",
    "    id_column = None\n",
    "    if 'id' in X.columns:\n",
    "        id_column = X['id'].copy()\n",
    "        X = X.drop(columns=['id'])\n",
    "    \n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nüîç Preprocessing {'training' if is_training else 'test'} data...\")\n",
    "    print(f\"  Numerical columns: {len(numerical_cols)}\")\n",
    "    print(f\"  Categorical columns: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Handle missing values in numerical columns (fill with median)\n",
    "    for col in numerical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            median_value = X[col].median()\n",
    "            X[col].fillna(median_value, inplace=True)\n",
    "            print(f\"  Filled {X[col].isnull().sum()} missing values in '{col}' with median: {median_value:.2f}\")\n",
    "    \n",
    "    # Handle missing values in categorical columns (fill with mode)\n",
    "    for col in categorical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            mode_value = X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown'\n",
    "            X[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  Filled {X[col].isnull().sum()} missing values in '{col}' with mode: {mode_value}\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    if feature_encoders is None:\n",
    "        feature_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if is_training:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            feature_encoders[col] = le\n",
    "        else:\n",
    "            if col in feature_encoders:\n",
    "                le = feature_encoders[col]\n",
    "                # Handle unseen categories\n",
    "                X[col] = X[col].astype(str).apply(lambda x: x if x in le.classes_ else le.classes_[0])\n",
    "                X[col] = le.transform(X[col])\n",
    "            else:\n",
    "                print(f\"  Warning: No encoder found for '{col}', using default encoding\")\n",
    "                X[col] = 0\n",
    "    \n",
    "    # Encode target variable\n",
    "    if is_training:\n",
    "        if target_encoder is None:\n",
    "            target_encoder = LabelEncoder()\n",
    "            y = target_encoder.fit_transform(y.astype(str))\n",
    "        else:\n",
    "            y = target_encoder.transform(y.astype(str))\n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    print(f\"‚úì Preprocessing complete\")\n",
    "    print(f\"  Final feature count: {len(feature_names)}\")\n",
    "    print(f\"  Final sample count: {len(X)}\")\n",
    "    \n",
    "    return X, y, target_encoder, feature_encoders, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# APPLY PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "X, y, target_encoder, feature_encoders, feature_names = preprocess_data(df, is_training=True)\n",
    "\n",
    "print(f\"\\n‚úì Data preprocessing completed\")\n",
    "print(f\"  Features shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "print(f\"  Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"  Class labels: {target_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Feature Engineering (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE ENGINEERING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def engineer_features(X_data, feature_names):\n",
    "    \"\"\"\n",
    "    Create additional features from existing ones.\n",
    "    \n",
    "    This is a placeholder function. You can add your own feature engineering here.\n",
    "    Examples:\n",
    "    - Interaction features\n",
    "    - Polynomial features\n",
    "    - Aggregation features\n",
    "    - Domain-specific features\n",
    "    \n",
    "    Args:\n",
    "        X_data: Feature matrix\n",
    "        feature_names: List of feature names\n",
    "    \n",
    "    Returns:\n",
    "        X_engineered: Feature matrix with additional features\n",
    "        new_feature_names: Updated list of feature names\n",
    "    \"\"\"\n",
    "    \n",
    "    X_eng = X_data.copy()\n",
    "    new_features = []\n",
    "    \n",
    "    # Example: Create interaction features (commented out)\n",
    "    # if 'Age at enrollment' in feature_names and 'Admission grade' in feature_names:\n",
    "    #     age_idx = feature_names.index('Age at enrollment')\n",
    "    #     grade_idx = feature_names.index('Admission grade')\n",
    "    #     X_eng['Age_x_Grade'] = X_data.iloc[:, age_idx] * X_data.iloc[:, grade_idx]\n",
    "    #     new_features.append('Age_x_Grade')\n",
    "    \n",
    "    # Example: Create polynomial features (commented out)\n",
    "    # if 'Admission grade' in feature_names:\n",
    "    #     grade_idx = feature_names.index('Admission grade')\n",
    "    #     X_eng['Admission_grade_squared'] = X_data.iloc[:, grade_idx] ** 2\n",
    "    #     new_features.append('Admission_grade_squared')\n",
    "    \n",
    "    new_feature_names = list(feature_names) + new_features\n",
    "    \n",
    "    if len(new_features) > 0:\n",
    "        print(f\"‚úì Created {len(new_features)} new features\")\n",
    "        print(f\"  New features: {new_features}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No feature engineering applied (placeholder function)\")\n",
    "    \n",
    "    return X_eng, new_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# APPLY FEATURE ENGINEERING (IF ENABLED)\n",
    "# ============================================================\n",
    "\n",
    "if ENABLE_FEATURE_ENGINEERING:\n",
    "    X, feature_names = engineer_features(X, feature_names)\n",
    "    print(f\"\\n‚úì Feature engineering completed\")\n",
    "    print(f\"  Total features: {len(feature_names)}\")\n",
    "else:\n",
    "    print(\"\\n‚äó Feature engineering disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SPLIT DATA INTO TRAINING AND VALIDATION SETS\n",
    "# ============================================================\n",
    "\n",
    "# Use stratified split to maintain class distribution\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Data split completed\")\n",
    "print(f\"  Training set size: {X_train.shape[0]} samples ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "print(f\"  Validation set size: {X_val.shape[0]} samples ({TEST_SIZE*100:.0f}%)\")\n",
    "print(f\"  Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Verify class distribution\n",
    "train_dist = np.bincount(y_train) / len(y_train)\n",
    "val_dist = np.bincount(y_val) / len(y_val)\n",
    "\n",
    "print(\"\\n  Class distribution:\")\n",
    "for i, class_name in enumerate(target_encoder.classes_):\n",
    "    print(f\"    {class_name}: Train={train_dist[i]:.2%}, Val={val_dist[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Model Training - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN XGBOOST MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ Training XGBoost model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Determine if binary or multiclass\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "if n_classes == 2:\n",
    "    objective = 'binary:logistic'\n",
    "    eval_metric = 'logloss'\n",
    "else:\n",
    "    objective = 'multi:softmax'\n",
    "    eval_metric = 'mlogloss'\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=500,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=objective,\n",
    "    eval_metric=eval_metric,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    num_class=n_classes if n_classes > 2 else None\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_xgb = accuracy_score(y_train, y_train_pred_xgb)\n",
    "val_acc_xgb = accuracy_score(y_val, y_val_pred_xgb)\n",
    "\n",
    "print(f\"‚úì XGBoost training completed\")\n",
    "print(f\"  Training accuracy: {train_acc_xgb:.4f}\")\n",
    "print(f\"  Validation accuracy: {val_acc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Model Training - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN LIGHTGBM MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ Training LightGBM model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Determine if binary or multiclass\n",
    "if n_classes == 2:\n",
    "    objective = 'binary'\n",
    "else:\n",
    "    objective = 'multiclass'\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=500,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=objective,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    num_class=n_classes if n_classes > 2 else None,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_lgb = lgb_model.predict(X_train)\n",
    "y_val_pred_lgb = lgb_model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_lgb = accuracy_score(y_train, y_train_pred_lgb)\n",
    "val_acc_lgb = accuracy_score(y_val, y_val_pred_lgb)\n",
    "\n",
    "print(f\"‚úì LightGBM training completed\")\n",
    "print(f\"  Training accuracy: {train_acc_lgb:.4f}\")\n",
    "print(f\"  Validation accuracy: {val_acc_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Model Training - CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN CATBOOST MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ Training CatBoost model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Automatically detect categorical features\n",
    "cat_features = []\n",
    "for i, col in enumerate(feature_names):\n",
    "    # Check if feature has low cardinality (likely categorical)\n",
    "    unique_values = X[col].nunique()\n",
    "    if unique_values <= 50 or unique_values <= max(20, int(0.05 * len(X))):\n",
    "        cat_features.append(i)\n",
    "\n",
    "print(f\"  Detected {len(cat_features)} categorical features\")\n",
    "\n",
    "# Determine loss function\n",
    "if n_classes == 2:\n",
    "    loss_function = 'Logloss'\n",
    "else:\n",
    "    loss_function = 'MultiClass'\n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    iterations=500,\n",
    "    subsample=0.8,\n",
    "    colsample_bylevel=0.8,\n",
    "    loss_function=loss_function,\n",
    "    eval_metric='Accuracy',\n",
    "    random_seed=RANDOM_SEED,\n",
    "    verbose=False,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "catboost_model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=cat_features,\n",
    "    eval_set=(X_val, y_val),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_cat = catboost_model.predict(X_train).ravel()\n",
    "y_val_pred_cat = catboost_model.predict(X_val).ravel()\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_cat = accuracy_score(y_train, y_train_pred_cat)\n",
    "val_acc_cat = accuracy_score(y_val, y_val_pred_cat)\n",
    "\n",
    "print(f\"‚úì CatBoost training completed\")\n",
    "print(f\"  Training accuracy: {train_acc_cat:.4f}\")\n",
    "print(f\"  Validation accuracy: {val_acc_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARE ALL THREE MODELS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä Model Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM', 'CatBoost'],\n",
    "    'Training Accuracy': [train_acc_xgb, train_acc_lgb, train_acc_cat],\n",
    "    'Validation Accuracy': [val_acc_xgb, val_acc_lgb, val_acc_cat]\n",
    "})\n",
    "\n",
    "# Add overfitting indicator\n",
    "comparison_df['Overfitting'] = comparison_df['Training Accuracy'] - comparison_df['Validation Accuracy']\n",
    "\n",
    "# Sort by validation accuracy\n",
    "comparison_df = comparison_df.sort_values('Validation Accuracy', ascending=False)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_val_acc = comparison_df.iloc[0]['Validation Accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Best model: {best_model_name} (Validation Accuracy: {best_val_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PERFORM CROSS-VALIDATION ON SELECTED MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüîç Performing {CV_FOLDS}-fold cross-validation on {SELECTED_MODEL.upper()}...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select the model based on configuration\n",
    "if SELECTED_MODEL == 'xgboost':\n",
    "    cv_model = xgb_model\n",
    "elif SELECTED_MODEL == 'lightgbm':\n",
    "    cv_model = lgb_model\n",
    "elif SELECTED_MODEL == 'catboost':\n",
    "    cv_model = catboost_model\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Invalid model selection: {SELECTED_MODEL}. Using XGBoost.\")\n",
    "    cv_model = xgb_model\n",
    "    SELECTED_MODEL = 'xgboost'\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "cv_scores = cross_val_score(cv_model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(f\"\\n‚úì Cross-validation completed\")\n",
    "print(f\"  CV Scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "print(f\"  Mean CV Accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Std CV Accuracy: {cv_scores.std():.4f}\")\n",
    "print(f\"  Mean ¬± Std: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HYPERPARAMETER TUNING FOR XGBOOST\n",
    "# ============================================================\n",
    "\n",
    "if ENABLE_HYPERPARAMETER_TUNING and SELECTED_MODEL == 'xgboost':\n",
    "    print(\"\\nüîß Performing hyperparameter tuning for XGBoost...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è This may take several minutes...\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.5]\n",
    "    }\n",
    "    \n",
    "    # Create base model\n",
    "    base_model = xgb.XGBClassifier(\n",
    "        objective=objective,\n",
    "        eval_metric=eval_metric,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        num_class=n_classes if n_classes > 2 else None\n",
    "    )\n",
    "    \n",
    "    # Perform randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        scoring='accuracy',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\n‚úì Hyperparameter tuning completed\")\n",
    "    print(f\"  Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"  Best CV score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Update model with best parameters\n",
    "    xgb_model = random_search.best_estimator_\n",
    "    \n",
    "elif ENABLE_HYPERPARAMETER_TUNING and SELECTED_MODEL == 'lightgbm':\n",
    "    print(\"\\nüîß Performing hyperparameter tuning for LightGBM...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è This may take several minutes...\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'num_leaves': [31, 50, 100]\n",
    "    }\n",
    "    \n",
    "    # Create base model\n",
    "    base_model = lgb.LGBMClassifier(\n",
    "        objective=objective,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        num_class=n_classes if n_classes > 2 else None,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # Perform randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        scoring='accuracy',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\n‚úì Hyperparameter tuning completed\")\n",
    "    print(f\"  Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"  Best CV score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Update model with best parameters\n",
    "    lgb_model = random_search.best_estimator_\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚äó Hyperparameter tuning disabled\")\n",
    "    if ENABLE_HYPERPARAMETER_TUNING:\n",
    "        print(f\"  Note: Hyperparameter tuning is only implemented for XGBoost and LightGBM\")\n",
    "        print(f\"  Current model: {SELECTED_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RETRAIN ON FULL DATASET\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüéØ Training final {SELECTED_MODEL.upper()} model on full dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select and train the final model\n",
    "if SELECTED_MODEL == 'xgboost':\n",
    "    final_model = xgb_model\n",
    "    final_model.fit(X, y, verbose=False)\n",
    "elif SELECTED_MODEL == 'lightgbm':\n",
    "    final_model = lgb_model\n",
    "    final_model.fit(X, y, callbacks=[lgb.log_evaluation(period=0)])\n",
    "elif SELECTED_MODEL == 'catboost':\n",
    "    final_model = catboost_model\n",
    "    final_model.fit(X, y, cat_features=cat_features, verbose=False)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Invalid model selection: {SELECTED_MODEL}\")\n",
    "    final_model = xgb_model\n",
    "\n",
    "print(f\"‚úì Final model training completed\")\n",
    "\n",
    "# Save the model\n",
    "model_filename = f'model_{SELECTED_MODEL}.pkl'\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(f\"‚úì Model saved as '{model_filename}'\")\n",
    "\n",
    "# Save the encoders\n",
    "encoders_filename = f'encoders_{SELECTED_MODEL}.pkl'\n",
    "joblib.dump({\n",
    "    'target_encoder': target_encoder,\n",
    "    'feature_encoders': feature_encoders\n",
    "}, encoders_filename)\n",
    "print(f\"‚úì Encoders saved as '{encoders_filename}'\")\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names),\n",
    "    'n_classes': n_classes,\n",
    "    'class_names': target_encoder.classes_.tolist(),\n",
    "    'cat_features': cat_features if SELECTED_MODEL == 'catboost' else []\n",
    "}\n",
    "\n",
    "feature_info_filename = f'feature_info_{SELECTED_MODEL}.json'\n",
    "with open(feature_info_filename, 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(f\"‚úì Feature info saved as '{feature_info_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14: Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE SUBMISSION FILE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüì§ Generating submission file...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if test data exists\n",
    "if Path(TEST_DATA_PATH).exists():\n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(TEST_DATA_PATH, sep=None, engine='python')\n",
    "    test_df.columns = test_df.columns.str.replace('\\t', ' ').str.strip()\n",
    "    \n",
    "    print(f\"‚úì Test data loaded: {test_df.shape}\")\n",
    "    \n",
    "    # Save test IDs if present\n",
    "    if 'id' in test_df.columns:\n",
    "        test_ids = test_df['id'].copy()\n",
    "    else:\n",
    "        test_ids = pd.Series(range(len(test_df)), name='id')\n",
    "    \n",
    "    # Preprocess test data (without target)\n",
    "    X_test, _, _, _, _ = preprocess_data(\n",
    "        test_df, \n",
    "        target_encoder=target_encoder,\n",
    "        feature_encoders=feature_encoders,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    if SELECTED_MODEL == 'catboost':\n",
    "        predictions = final_model.predict(X_test).ravel()\n",
    "    else:\n",
    "        predictions = final_model.predict(X_test)\n",
    "    \n",
    "    # Decode predictions back to original labels\n",
    "    predictions_decoded = target_encoder.inverse_transform(predictions.astype(int))\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'Target': predictions_decoded\n",
    "    })\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_filename = f'submission_{SELECTED_MODEL}.csv'\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Submission file generated: '{submission_filename}'\")\n",
    "    print(f\"  Number of predictions: {len(submission_df)}\")\n",
    "    \n",
    "    # Display first 10 predictions\n",
    "    print(\"\\n  First 10 predictions:\")\n",
    "    print(submission_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Show prediction distribution\n",
    "    print(\"\\n  Prediction distribution:\")\n",
    "    pred_dist = submission_df['Target'].value_counts()\n",
    "    for label, count in pred_dist.items():\n",
    "        print(f\"    {label}: {count} ({count/len(submission_df)*100:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Test data file not found: {TEST_DATA_PATH}\")\n",
    "    print(\"  Skipping submission generation\")\n",
    "    print(\"  To generate submission, place test.csv in the current directory and re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 15: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä FINAL MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüèÜ Model Details:\")\n",
    "print(f\"  Selected Model: {SELECTED_MODEL.upper()}\")\n",
    "print(f\"  Number of Features: {len(feature_names)}\")\n",
    "print(f\"  Number of Classes: {n_classes}\")\n",
    "print(f\"  Class Names: {', '.join(target_encoder.classes_)}\")\n",
    "\n",
    "print(f\"\\nüìà Performance Metrics:\")\n",
    "if SELECTED_MODEL == 'xgboost':\n",
    "    print(f\"  Training Accuracy: {train_acc_xgb:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_acc_xgb:.4f}\")\n",
    "elif SELECTED_MODEL == 'lightgbm':\n",
    "    print(f\"  Training Accuracy: {train_acc_lgb:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_acc_lgb:.4f}\")\n",
    "elif SELECTED_MODEL == 'catboost':\n",
    "    print(f\"  Training Accuracy: {train_acc_cat:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_acc_cat:.4f}\")\n",
    "\n",
    "print(f\"  Mean CV Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Files:\")\n",
    "print(f\"  Model: {model_filename}\")\n",
    "print(f\"  Encoders: {encoders_filename}\")\n",
    "print(f\"  Feature Info: {feature_info_filename}\")\n",
    "if Path(TEST_DATA_PATH).exists():\n",
    "    print(f\"  Submission: submission_{SELECTED_MODEL}.csv\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Configuration Used:\")\n",
    "print(f\"  Hyperparameter Tuning: {ENABLE_HYPERPARAMETER_TUNING}\")\n",
    "print(f\"  Feature Engineering: {ENABLE_FEATURE_ENGINEERING}\")\n",
    "print(f\"  Visualizations: {ENABLE_VISUALIZATIONS}\")\n",
    "print(f\"  Random Seed: {RANDOM_SEED}\")\n",
    "print(f\"  CV Folds: {CV_FOLDS}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì Pipeline execution completed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tips for better performance\n",
    "print(\"\\nüí° Tips for Better Performance:\")\n",
    "print(\"  1. Try all three models and compare their CV scores\")\n",
    "print(\"  2. Enable hyperparameter tuning for fine-tuning\")\n",
    "print(\"  3. Experiment with feature engineering\")\n",
    "print(\"  4. Consider ensemble methods (averaging predictions)\")\n",
    "print(\"  5. Always trust cross-validation scores over validation scores\")\n",
    "print(\"\\nGood luck with your submission! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
